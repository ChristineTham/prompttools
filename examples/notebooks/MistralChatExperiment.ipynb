{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a13ddc8",
   "metadata": {},
   "source": [
    "# Mistral Chat Experiment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f0cfe",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885dabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edba05a",
   "metadata": {},
   "source": [
    "First, we'll need to set our API keys. If we are in DEBUG mode, we don't need to use a real OpenAI key, so for now we'll set them to empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"\"  # Insert your key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1e47",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beaa70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.experiment import MistralChatCompletionExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Run an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. We can iterate over models, inputs, and configurations like temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9114cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"mistral-tiny\"]\n",
    "messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who was the first president?\"},\n",
    "    ]\n",
    "]\n",
    "temperatures = [0.0, 1.0]\n",
    "# You can add more parameters that you'd like to test here.\n",
    "\n",
    "experiment = MistralChatCompletionExperiment(models, messages, temperature=temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa5450",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b33130",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7598332b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>messages</th>\n",
       "      <th>temperature</th>\n",
       "      <th>response</th>\n",
       "      <th>response_usage</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>{'completion_tokens': 18, 'prompt_tokens': 57, 'total_tokens': 75}</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>{'completion_tokens': 18, 'prompt_tokens': 57, 'total_tokens': 75}</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>{'completion_tokens': 18, 'prompt_tokens': 57, 'total_tokens': 75}</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>{'completion_tokens': 18, 'prompt_tokens': 57, 'total_tokens': 75}</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  \\\n",
       "0  gpt-3.5-turbo        \n",
       "1  gpt-3.5-turbo        \n",
       "2  gpt-3.5-turbo-0613   \n",
       "3  gpt-3.5-turbo-0613   \n",
       "\n",
       "                                                                                                                       messages  \\\n",
       "0  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]   \n",
       "1  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]   \n",
       "2  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]   \n",
       "3  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Who was the first president?'}]   \n",
       "\n",
       "   temperature           response  \\\n",
       "0  0.0          George Washington   \n",
       "1  1.0          George Washington   \n",
       "2  0.0          George Washington   \n",
       "3  1.0          George Washington   \n",
       "\n",
       "                                                       response_usage  \\\n",
       "0  {'completion_tokens': 18, 'prompt_tokens': 57, 'total_tokens': 75}   \n",
       "1  {'completion_tokens': 18, 'prompt_tokens': 57, 'total_tokens': 75}   \n",
       "2  {'completion_tokens': 18, 'prompt_tokens': 57, 'total_tokens': 75}   \n",
       "3  {'completion_tokens': 18, 'prompt_tokens': 57, 'total_tokens': 75}   \n",
       "\n",
       "    latency  \n",
       "0  0.000006  \n",
       "1  0.000005  \n",
       "2  0.000003  \n",
       "3  0.000002  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluate the model response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "To evaluate the results, we'll define an eval function. We can use semantic distance to check if the model's response is similar to our expected output.\n",
    "\n",
    "Since we are using semantic similarity, you need to have the library `sentence_transformers` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
